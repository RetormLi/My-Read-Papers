# 背景
数据集使用HotpotQA，总体上参考了一些多跳推理和图结构的工作。如Coref-GRN，Memory Networks等。总的来说，图结构推理的一些工作展现出了不错的成果。
但是这些模型往往在单一类型、实体有限的数据集上表现良好，不适合大型的复杂数据集。并且HotpotQA不提供所需的实体级的强监督。
在实现中也使用了一些前人的成果。比如在多个地方使用的Bi-Attention，BERT，以及一些计算方式(GAT的attention score计算)，预测时参考hotpotQA的baseline model，等。
# 问题
着力解决复杂数据集带来的离散证据和多跳推理问题。希望能够更可解释地实现推理（如加入图结构），模仿人的推理过程，并且对于大量的实体能够有效删除不需要的结点（mask layer）。
# 动机
之前的多跳推理模型比较难以解释。而图结构的多跳推理是比较符合直觉的形式。在推理的过程中，作者用一个Fusion Block来模仿人的一步推理过程（这里有一些问题）。而在删除多余结点方面，作者使用由attention生成的mask去除关系较小的结点，有点类似图搜索的剪枝（这里也有一些问题）。
# 方法或模型
整个模型的结构仍然是 embed -> 改善建构embeddings -> classify and predict 的模式。（这里也有一些问题）   

1. Paragraph Selection。由于文本的规模很大，需要先选出可能与之相关的文段。利用pre-trained BERT计算相关度。
2. Graph Constructor。对于同句子、同语境、中心实体和同文段其他实体间建立边。
3. Encoder。Bi-Attetion+BERT。输出C0、Q0两个矩阵。
4. Fusion Block。主要进行推理过程。
5. Predicton Layer。采用和HotpotQA基本相同的方法，输出三个：support，span，type。Loss函数为start, end, support, type的loss的线性组合。


**Fusion Block**
- Tok2Ent (Doc2Graph)：由token表示转换为entity表示。
- Soft Mask：通过query和entity的attention来mask一些无关entity。（每一步都贪心法剪枝，岂不是和单跳没有太大区别）
- Dynamic Graph Attention：通过图结构向neighbors(entitys)传播信息（不太明白是传播什么信息）
- Graph2Doc/Updating Query：通过更新后的entitys更新context和query的表示，并且准备输出或者进行下一步的推理。
      
# 试验
使用2-layer fusion blocks。在answer正确率上提高较多，但在support fact上表现一般。
在消融试验中，各个部分均有作用。奇妙的是，作为一个针对多跳的模型，fusion blocks的影响要小于BiAttention。噪声对性能的影响很小，以及在仅有关键文段作为输入的测试中，并没有提高太多性能。
另外，作者又提出了一种基于路径的评价方法。可以看出在推理过程中attended entity的变化情况。
# 粗浅认识
可能由于我知识储备尚少，觉得这篇文章颇令人疑惑。在想象中的理想图结构多跳阅读应该是建构基于图结构的文本表示，然后在图结构中进行搜索推理路径。而这篇文章则是用图结构对文本进行建构，其中加入mask等来去除文本的多余信息。但整体结构上仍然是先用图embed再改良再分类的思想，和图推理的关系不是很大。
在每次mask时，都是entity和query进行attetion。然而，多跳即是一步比较无法得出信息和问题的联系，而需要多步推理。这种剪枝方式更像是贪心搜索，推理一次而不知道结果时，就直接对于attetion score低的结点进行剪枝（mask），略感粗暴，在这里个人觉得不符合多跳的思维。
模型并非在图中搜索，而是搜索的深度由block的个数决定，这个数字是在训练时即确定了。意味着这种图搜索是较为盲目的，进行事先设定好的推理步骤，每一步推理都是贪心搜索，只利用上一次搜索的信息，而无法得知整条路径的信息。同时也无法自动判断一个问题需要多少步的搜索才能达到。
依此联想整篇文章的重点也就只是在如何更好地表示文本，而非真的在用图来进行推理。这一点在模型的结构上也能看出来，整体结构不变，而只是在生成富信息的文本表示上做出创新。这一结构即与人推理时的常态不同。正常的推理应该从初步embed和分类开始，然后对实体关系层层深入，直到达到推理终点。这里的推理从embed开始，然后将query加入文本表示，删除多余信息，然后再去分类和预测。个人觉得这样是不合理的。
总的来说，除了在最后评估时使用的BFS，我觉得绝大多数地方都没有使用图结构。其中最重要的Dynamic Graph Attention，动态的带权图的关联矩阵，Attention矩阵也完全可以将其替代。而真正人推理中要使用的图搜索（也是Attention矩阵中所不具备的），在整个过程中并没有使用。那么这样的结构真的有使用图结构的必要吗？
另外，在一些用语的使用上我并没有完全理解，比如neighbor是entity上邻近的还是token上临近的，heuristic mask又是从哪里来的，等等。这些细节似乎论文里没有解释清楚。
消融实验中发生的情况也令人疑惑。影响最小的便是entity mask。而BiAttetion对于性能的影响如此之大难以理解。
总之，我觉得以这种方式进行图推理不太合适。
有一个很关键的地方，就是对于离散形式的推理过程是不可导的。
